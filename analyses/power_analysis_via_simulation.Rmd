---
title: "Power analyses for Experiment 7"
subtitle: "Using data from the previous experiments"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# To do

- add tests of H4
- explicate that power analyses not conducted for H3

```{r include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

# Setup

```{r}

# dependencies
library(tidyverse)
library(simglm)
library(brms)
library(bayestestR)
library(tidybayes)
library(future)
library(furrr)
library(knitr)
library(kableExtra)

# set up parallelization 
plan(multiprocess)

# create necessary folder
dir.create("models")

```

# Data

- Calculate empirical exclusion rate from existing data.
- Load fitted meta-analyses models of existing data (fitted in the exploratory analyses script).

```{r}

# exclusion rate
data_exclusion_rate <- read.csv("../data/processed/4_data_participant_level_with_hand_scoring.csv") %>%
  # set factor levels for t test comparisons
  mutate(exclude = ifelse(exclude_subject == FALSE & exclude_implausible_intervention_linger == FALSE, FALSE, TRUE)) %>%
  summarize(percent_excluded = mean(exclude)) %>%
  pull(percent_excluded)

# empirical fits (created by exploratory analyses script)
fit_selfreport <- read_rds("models/fit_selfreport.rds")
fit_implicit   <- read_rds("models/fit_implicit.rds")
fit_intentions <- read_rds("models/fit_intentions.rds")

```

# Hyper parameters

```{r}

N_participants <- 400
N_simulations <- 1000

```

- N participants to be simulated = `r N_participants`
- N simulation iterations run = `r N_simulations`

# Functions

Define the functions used to run the simulations.

- simulate_data(): Simulate data that mimics to the empirical data, using the fitted empirical models.
- fit_initial_model(): Fit the model to this newly simulated data, check for parameter recovery. 
- fit_and_hypothesis_test(): Repeatedly (1) simulate new data again (calling the above function), (2) fit the model to this simulated data (by updating the fit_initial model above, in order to reuse its STAN code and therefore save a lot of run time), and (3) apply our hypothesis tests to the results of these fits.
- run_simulation(): Run fit_and_hypothesis_test() a large number of times, append and tidy its results into a tibble.

```{r}

simulate_data <- function(fitted_empirical_model, N){
  
  # get point estimates from fitted data
  results_empirical <- as_tibble(map_estimate(fitted_empirical_model))
  
  # extract these point estimates
  beta_Intercept <- results_empirical %>% 
    filter(Parameter == "b_Intercept") %>% 
    pull(MAP_Estimate)
  
  beta_source_valence <- results_empirical %>% 
    filter(Parameter == "b_source_valencepositive") %>% 
    pull(MAP_Estimate)
  
  beta_experiment_condition <- results_empirical %>% 
    filter(Parameter == "b_experiment_conditiondeepfaked") %>%
    pull(MAP_Estimate)
  
  beta_interaction <- results_empirical %>% 
    filter(Parameter == "b_source_valencepositive.experiment_conditiondeepfaked") %>% 
    pull(MAP_Estimate)
  
  # set parameters of model to be simulated using point estimates
  simulation_arguments <- 
    list(formula     = DV ~ 1 + source_valence + experiment_condition + interaction,
         reg_weights = c(beta_Intercept, beta_source_valence, beta_experiment_condition, beta_interaction),
         fixed       = list(source_valence       = list(var_type = "continuous", mean = 0, sd = 2),
                            experiment_condition = list(var_type = "continuous", mean = 0, sd = 2),
                            interaction          = list(var_type = "continuous", mean = 0, sd = 2)),
         sample_size = N)
  
  # simulate data to fit_initial model
  data_simulated <- simulation_arguments %>%
    simulate_fixed(data = NULL, .) %>%
    simulate_error(simulation_arguments) %>%
    generate_response(simulation_arguments)
  
  return(data_simulated)
  
}


fit_initial_model <- function(fitted_empirical_model, N, filename){
  
  # NB it is faster to simulate and fit the model once, then use update() this fit, rather than generating the STAN code for the model at every iteration. 
  
  # simulate data using fitted model
  simulated_data <- simulate_data(fitted_empirical_model, N = N)
  
  # fit the same model to the simulated data
  fit_initial <- 
    brm(formula = DV ~ 1 + source_valence + experiment_condition + interaction,
        data    = simulated_data,
        prior   = prior(normal(0, 10)),
        iter    = 10000,
        warmup  = 3000,
        control = list(adapt_delta = 0.99),  # to avoid divergent transitions
        chains  = 4,
        cores   = parallel::detectCores(),
        file    = paste0("models/", filename))
  
  # sanity check: can we recover comparable parameters that we built the model with?
  # e.g., the fixed effect estimates should be close to the reg_weights used above.
  # get point estimates from fitted data
  results_empirical <- as_tibble(map_estimate(fitted_empirical_model))
  
  results_comparison <- results_empirical %>%
    mutate(Parameter = case_when(Parameter == "b_source_valencepositive" ~ "b_source_valence",
                                 Parameter == "b_experiment_conditiondeepfaked" ~ "b_experiment_condition",
                                 Parameter == "b_source_valencepositive.experiment_conditiondeepfaked" ~ "b_interaction",
                                 TRUE ~ Parameter)) %>%
    rename(empirical = MAP_Estimate) %>%
    full_join(fit_initial %>%
                map_estimate() %>%
                as_tibble() %>%
                rename(simulated = MAP_Estimate), by = "Parameter") %>%
    mutate_if(is.numeric, round, digits = 2)
  
  return(list(fitted_empirical_model = fitted_empirical_model,
              N = N,
              fit_initial = fit_initial,
              results_comparison = results_comparison))
  
}


fit_and_hypothesis_test <- function(fit_initial_model_object, seed){
  
  # set seed
  set.seed(seed)
  
  # simulate data using fitted model
  simulated_data <- simulate_data(fitted_empirical_model = fit_initial_model_object$fitted_empirical_model, 
                                  N = fit_initial_model_object$N)
  
  # find a new (updated) fit using the newly simulated data
  fit_updated <- update(fit_initial_model_object$fit_initial, newdata = simulated_data)
  
  # manipulate posterior distributions
  draws_fit_updated <-
    bind_cols(select(spread_draws(fit_updated, b_Intercept),            b_Intercept),
              select(spread_draws(fit_updated, b_source_valence),       b_source_valence),
              select(spread_draws(fit_updated, b_experiment_condition), b_experiment_condition),
              select(spread_draws(fit_updated, b_interaction),          b_interaction)) %>%
    mutate(effect_genuine   = b_source_valence,
           effect_deepfaked = b_source_valence + b_experiment_condition + b_interaction)
  
  # parameterize posteriors
  estimates_fit_updated <-
    # 95% CIs
    (bayestestR::hdi(draws_fit_updated, ci = .95) %>% 
       rename(CI_95_lower = CI_low) %>%
       as_tibble()) %>%
    # 90% CIs (for non-inferiority test)
    full_join(bayestestR::hdi(draws_fit_updated, ci = .90) %>%
                as_tibble() %>%
                rename(CI_90_lower = CI_low),
              by = "Parameter") %>%
    select(Parameter, CI_95_lower, CI_90_lower)
  
  # lower CI estimates hypothesis tests
  effect_genuine_95_CI_lower   <- estimates_fit_updated %>% filter(Parameter == "effect_genuine")   %>% pull(CI_95_lower)
  effect_deepfaked_95_CI_lower <- estimates_fit_updated %>% filter(Parameter == "effect_deepfaked") %>% pull(CI_95_lower)
  effect_deepfaked_90_CI_lower <- estimates_fit_updated %>% filter(Parameter == "effect_deepfaked") %>% pull(CI_90_lower)
  
  ## non-zero effect in genuine condition
  H1_genuine <- ifelse(effect_genuine_95_CI_lower > 0, 1, 0)
  ## non-zero effect in deepfake condition
  H1_deepfaked <- ifelse(effect_deepfaked_95_CI_lower > 0, 1, 0)
  ## non-inferiority of deepfakes compared to genuine
  H2_noninferiority <- ifelse(effect_genuine_95_CI_lower < effect_deepfaked_90_CI_lower, 1, 0)
  
  # binary hypothesis tests
  results_simulation <- tibble(effect_genuine_95_CI_lower = effect_genuine_95_CI_lower,
                               effect_deepfaked_95_CI_lower = effect_deepfaked_95_CI_lower,
                               effect_deepfaked_90_CI_lower = effect_deepfaked_90_CI_lower,
                               H1_genuine = H1_genuine,
                               H1_deepfaked = H1_deepfaked,
                               H2_noninferiority = H2_noninferiority)
  
  return(results_simulation)
  
}


run_simulation <- function(fit_initial_model_object, n_sim){
  
  results <- 
    tibble(seed = 1:n_sim) %>% 
    mutate(tidy = future_map(seed, fit_and_hypothesis_test, 
                             .options = furrr_options(seed = TRUE),
                             .progress = TRUE,
                             fit_initial_model_object = fit_initial_model_object)) %>%
    unnest(tidy) 
  
  return(results)
  
}

```

# Check parameter recovery

As a sanity check to ensure that (1) models are being extracted correctly from the original (meta analytic) empirical fits, (2) data fitting these models is being simulated correctly, and (3) (fixed effect) models are being fit to these simulated data correctly, ensure that parameters from the empirical models are being recovered.

```{r}

parameter_recovery_self_report <- fit_initial_model(fitted_empirical_model = fit_selfreport, 
                                                    filename = "fit_simulation_selfreport",
                                                    N = N_participants)

parameter_recovery_self_report$results_comparison %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE)

```

# Run simulations

For each DV and their hypothesis. Run time of an hour or more on good hardware, depending on parameters.

```{r}

# self report
if(file.exists("models/simulation_iterations_self_report.csv")){
  simulation_iterations_self_report <- read_csv("models/simulation_iterations_self_report.csv")
} else {
  # run simulations
  simulation_iterations_self_report <- fit_selfreport %>%
    fit_initial_model(fitted_empirical_model = ., 
                      filename = "fit_simulation_selfreport", 
                      N = N_participants) %>%
    run_simulation(fit_initial_model_object = ., 
                   n_sim = N_simulations)
  # write to disk
  simulation_iterations_self_report %>% 
    write_csv("models/simulation_iterations_self_report.csv")
}

# implicit
if(file.exists("models/simulation_iterations_implicit.csv")){
  simulation_iterations_implicit <- read_csv("models/simulation_iterations_implicit.csv")
} else {
  # run simulations
  simulation_iterations_implicit <- fit_implicit %>%
    fit_initial_model(fitted_empirical_model = ., 
                      filename = "fit_simulation_implicit", 
                      N = N_participants) %>%
    run_simulation(fit_initial_model_object = ., 
                   n_sim = N_simulations)
  # write to disk
  simulation_iterations_implicit %>% 
    write_csv("models/simulation_iterations_implicit.csv")
}

# behavioral intentions
if(file.exists("models/simulation_iterations_intentions.csv")){
  simulation_iterations_intentions <- read_csv("models/simulation_iterations_intentions.csv")
} else {
  # run simulations
  simulation_iterations_intentions <- fit_intentions %>%
    fit_initial_model(fitted_empirical_model = ., 
                      filename = "fit_simulation_intentions", 
                      N = N_participants) %>%
    run_simulation(fit_initial_model_object = ., 
                   n_sim = N_simulations)
  # write to disk
  simulation_iterations_intentions %>% 
    write_csv("models/simulation_iterations_intentions.csv")
}

```

# Results

Based on the meta-analyses of our experiments so far, and taking the exclusion rate into account (i.e., `r round(data_exclusion_rate*100, 1)`%), a sample size of $N \geq$ `r ceiling(N_participants / (1 - data_exclusion_rate))` would provide the following degree of statistical power to test each hypothesis:

NB hypothesis labels follow the preregistration document and the analysis script.

```{r fig.height=5, fig.width=12}

# combine 
simuations_iterations <- 
  bind_rows(simulation_iterations_self_report %>% mutate(DV = "Self report"),
            simulation_iterations_implicit %>% mutate(DV = "Implicit"),
            simulation_iterations_intentions %>% mutate(DV = "Intentions")) %>%
  mutate(DV = fct_relevel(DV, "Self report", "Implicit", "Intentions"))

# table
simuations_iterations %>%
  select(DV, H1_genuine, H1_deepfaked, H2_noninferiority) %>%
  pivot_longer(cols = c(H1_genuine, H1_deepfaked, H2_noninferiority),
               names_to = "detail",
               values_to = "detected") %>%
  group_by(DV, detail) %>%
  summarize(power = mean(detected), .groups = "drop") %>%
  mutate(hypothesis = paste(DV, detail, sep = "_"),
         hypothesis = case_when(hypothesis == "Self report_H1_genuine" ~ "H1a",
                                hypothesis == "Self report_H1_deepfaked" ~ "H1b",
                                hypothesis == "Self report_H2_noninferiority" ~ "H2a",
                                hypothesis == "Implicit_H1_genuine" ~ "H1c",
                                hypothesis == "Implicit_H1_deepfaked" ~ "H1d",
                                hypothesis == "Implicit_H2_noninferiority" ~ "H2b",
                                hypothesis == "Intentions_H1_genuine" ~ "H1e",
                                hypothesis == "Intentions_H1_deepfaked" ~ "H1f",
                                hypothesis == "Intentions_H2_noninferiority" ~ "H2c")) %>%
  select(hypothesis, DV, detail, power) %>%
  arrange(hypothesis) %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# H1 plot
simuations_iterations %>%
  select(DV, 
         Genuine   = effect_genuine_95_CI_lower,
         Deepfaked = effect_deepfaked_95_CI_lower) %>%
  pivot_longer(cols      = c(Genuine, Deepfaked),
               names_to  = "Condition",
               values_to = "value") %>%
  ggplot(aes(value, color = Condition, fill = Condition)) +
  geom_density(alpha = 0.2) +
  scale_color_viridis_d(begin = 0.3, end = 0.7) +
  scale_fill_viridis_d(begin = 0.3, end = 0.7) +
  xlab("Lower bound 95% CI") +
  facet_wrap(~ DV, scales = "free") + 
  theme(legend.position = "bottom") +
  ggtitle("H1")

# H2 plot
simuations_iterations %>%
  select(DV, 
         Genuine   = effect_genuine_95_CI_lower,
         Deepfaked = effect_deepfaked_90_CI_lower) %>%
  mutate(Inference = ifelse(Genuine < Deepfaked, "Evidence of non-inferiority", "No evidence of non-inferiority")) %>%
  ggplot(aes(Genuine, Deepfaked, color = Inference)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, linetype = "dashed") +
  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +
  xlab("Lower bound 95% CI for genuine condition") +
  ylab("Lower bound 90% CI\nfor Deepfaked condition") +
  facet_wrap(~ DV, scales = "free") + 
  theme(legend.position = "bottom") +
  ggtitle("H2")

```

# Resources

I used the [{simglm} package's vignette](https://cran.r-project.org/web/packages/simglm/vignettes/tidy_simulation.html) and [Solomon Kurz's blog post on Bayesian power analysis using {tidyverse} and {brms}](https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/) to create this simulation: many thanks to both.

# Session info

```{r}

sessionInfo()

```



