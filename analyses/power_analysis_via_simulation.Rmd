---
title: "Power analyses for Experiment 7"
subtitle: "Using data from the previous experiments"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

Note that this script requires the exploratory analyses script to be run first, so that the /models/ directory has the model fit objects for the empirical data present in it.

```{r include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

# Setup

```{r}

# dependencies
library(tidyverse)
library(simglm)
library(brms)
library(bayestestR)
library(tidybayes)
library(future)
library(furrr)
library(knitr)
library(kableExtra)

# set up parallelization 
plan(multiprocess)

# create necessary folder
dir.create("models")

```

# Data and fitted models

- Calculate empirical exclusion rate from existing data.
- Load fitted meta-analyses models of existing data (fitted in the exploratory analyses script).

```{r}

# empirical data
data_processed <- read.csv("../data/processed/4_data_participant_level_with_hand_scoring.csv") 

empirical_exclusion_rate <- data_processed %>%
  mutate(exclude = ifelse(exclude_subject == FALSE & 
                            exclude_implausible_intervention_linger == FALSE, FALSE, TRUE)) %>%
  summarize(percent_excluded = mean(exclude)) %>%
  pull(percent_excluded)

empirical_detection_rate <- data_processed %>%
  filter(exclude_subject == FALSE & 
           exclude_implausible_intervention_linger == FALSE &
           !is.na(deepfake_detection_open_recoded)) %>%
  summarize(percent_detected = mean(deepfake_detection_open_recoded)) %>%
  pull(percent_detected) 

# empirical fits (created by exploratory analyses script)
fit_selfreport <- read_rds("models/fit_selfreport.rds")
fit_implicit   <- read_rds("models/fit_implicit.rds")
fit_intentions <- read_rds("models/fit_intentions.rds")
# fit_selfreport_deepfaked_detected <- read_rds("models/fit_selfreport_deepfaked_detected.rds")
# fit_implicit_deepfaked_detected   <- read_rds("models/fit_implicit_deepfaked_detected.rds")
# fit_intentions_deepfaked_detected <- read_rds("models/fit_intentions_deepfaked_detected.rds")

```

# Parameters

Note that total N_participants is intended to represent the total sample after exclusions. This number is later adjusted upwards for the empirical exclusion rate to make a data collection sample size recommendation. It is also adjusted downward for the empirical deepfake detection and deepfake concept awareness rates when simulating power for the tests that will be run in these subsets. That is, only one sample size has to be set here, necessary adjustments to this are later done in code.  

```{r}

N_participants <- 100  # this value is changed across runs to observe its effect on the estimated power. 
N_simulations  <- 100  # increase to 1000+ after testing

```

- N participants to be simulated = `r N_participants`
- N simulation iterations run = `r N_simulations`

# Functions

Define the functions used to run the simulations.

- simulate_data(): Simulate data that mimics to the empirical data, using the fitted empirical models.
- fit_initial_model(): Fit the model to this newly simulated data, check for parameter recovery. 
- simulate_data_update_fit_and_hypothesis_test(): Repeatedly (1) simulate new data again (calling the above function), (2) fit the model to this simulated data (by updating the fit_initial model above, in order to reuse its STAN code and therefore save a lot of run time), and (3) apply our hypothesis tests to the results of these fits.
- run_multiple_iterations(): Run simulate_data_update_fit_and_hypothesis_test() a large number of times, append and tidy its results into a tibble.
- simulate(): Check if results from a previous run of the simulation have been saved to disk and read in if so. If no, run the simulation functions above in order.

```{r}

simulate_data <- function(fitted_empirical_model, N){
  
  # get point estimates from fitted data
  results_empirical <- as_tibble(map_estimate(fitted_empirical_model))
  
  # extract these point estimates
  beta_Intercept <- results_empirical %>% 
    filter(Parameter == "b_Intercept") %>% 
    pull(MAP_Estimate)
  
  beta_source_valence <- results_empirical %>% 
    filter(Parameter == "b_source_valencepositive") %>% 
    pull(MAP_Estimate)
  
  beta_experiment_condition <- results_empirical %>% 
    filter(Parameter == "b_experiment_conditiondeepfaked") %>%
    pull(MAP_Estimate)
  
  beta_interaction <- results_empirical %>% 
    filter(Parameter == "b_source_valencepositive.experiment_conditiondeepfaked") %>% 
    pull(MAP_Estimate)
  
  # set parameters of model to be simulated using point estimates
  simulation_arguments <- 
    list(formula     = DV ~ 1 + source_valence * experiment_condition,
         reg_weights = c(beta_Intercept, beta_source_valence, beta_experiment_condition, beta_interaction),
         fixed       = list(source_valence       = list(var_type = "factor", levels = c("negative", "positive")),
                            experiment_condition = list(var_type = "factor", levels = c("genuine", "deepfaked"))),
         sample_size = N)
  
  # simulate data to fit_initial model
  data_simulated <- simulation_arguments %>%
    simulate_fixed(data = NULL, .) %>%
    simulate_error(simulation_arguments) %>%
    generate_response(simulation_arguments)
  
  return(data_simulated)
  
}


fit_initial_model <- function(fitted_empirical_model, N, filename){
  
  # NB it is faster to simulate and fit the model once, then use update() this fit, rather than generating the STAN code for the model at every iteration. 
  
  # simulate data using fitted model
  simulated_data <- simulate_data(fitted_empirical_model, N = N)
  
  # fit the same model to the simulated data
  fit_initial <- 
    brm(formula = DV ~ 1 + source_valence * experiment_condition,
        data    = simulated_data,
        prior   = prior(normal(0, 10)),
        iter    = 10000,
        warmup  = 3000,
        control = list(adapt_delta = 0.99),  # to avoid divergent transitions
        chains  = 4,
        cores   = parallel::detectCores(),
        file    = paste0("models/", filename))
  
  # sanity check: can we recover comparable parameters that we built the model with?
  # e.g., the fixed effect estimates should be close to the reg_weights used above.
  # get point estimates from fitted data
  results_empirical <- fitted_empirical_model %>%
    map_estimate() %>%
    as_tibble() %>%
    mutate(Parameter = case_when(
      Parameter == "b_source_valencepositive" ~ "b_source_valence",
      Parameter == "b_experiment_conditiondeepfaked" ~ "b_experiment_condition",
      Parameter == "b_source_valencepositive.experiment_conditiondeepfaked" ~ "b_interaction",
      TRUE ~ Parameter)
    ) %>%
    rename(empirical = MAP_Estimate)
  
  results_simulated <- fit_initial %>%
    map_estimate() %>%
    as_tibble() %>%
    mutate(Parameter = case_when(
      Parameter == "b_source_valence.experiment_condition" ~ "b_interaction",
      TRUE ~ Parameter)
    ) %>%
    rename(simulated = MAP_Estimate)
  
  results_comparison <- full_join(results_empirical, results_simulated, by = "Parameter") 
  
  return(list(fitted_empirical_model = fitted_empirical_model,
              N = N,
              fit_initial = fit_initial,
              results_comparison = results_comparison))
  
}


simulate_data_update_fit_and_hypothesis_test <- function(fit_initial_model_object, seed){
  
  # set seed
  set.seed(seed)
  
  # simulate data using fitted model
  simulated_data <- simulate_data(fitted_empirical_model = fit_initial_model_object$fitted_empirical_model, 
                                  N = fit_initial_model_object$N)
  
  # find a new (updated) fit using the newly simulated data
  fit_updated <- update(fit_initial_model_object$fit_initial, newdata = simulated_data)
  
  # manipulate posterior distributions
  draws_fit_updated <-
    bind_cols(select(spread_draws(fit_updated, b_Intercept),            b_Intercept),
              select(spread_draws(fit_updated, b_source_valence),       b_source_valence),
              select(spread_draws(fit_updated, b_experiment_condition), b_experiment_condition),
              select(spread_draws(fit_updated, `b_source_valence:experiment_condition`),
                     b_interaction = `b_source_valence:experiment_condition`)) %>%
    mutate(effect_genuine   = b_source_valence,
           effect_deepfaked = b_source_valence + b_experiment_condition + b_interaction)
  
  # parameterize posteriors
  estimates_fit_updated <-
    # 95% CIs
    (bayestestR::hdi(draws_fit_updated, ci = .95) %>% 
       rename(CI_95_lower = CI_low) %>%
       as_tibble()) %>%
    # 90% CIs (for non-inferiority test)
    full_join(bayestestR::hdi(draws_fit_updated, ci = .90) %>%
                as_tibble() %>%
                rename(CI_90_lower = CI_low),
              by = "Parameter") %>%
    select(Parameter, CI_95_lower, CI_90_lower)
  
  # lower CI estimates hypothesis tests
  effect_genuine_95_CI_lower <- estimates_fit_updated %>% 
    filter(Parameter == "effect_genuine") %>% 
    pull(CI_95_lower)
  
  effect_deepfaked_95_CI_lower <- estimates_fit_updated %>% 
    filter(Parameter == "effect_deepfaked") %>% 
    pull(CI_95_lower)
  
  effect_deepfaked_90_CI_lower <- estimates_fit_updated %>% 
    filter(Parameter == "effect_deepfaked") %>% 
    pull(CI_90_lower)
  
  ## non-zero effect in genuine condition
  H1_genuine <- ifelse(effect_genuine_95_CI_lower > 0, 1, 0)
  ## non-zero effect in deepfake condition
  H1_deepfaked <- ifelse(effect_deepfaked_95_CI_lower > 0, 1, 0)
  ## non-inferiority of deepfakes compared to genuine
  H2_noninferiority <- ifelse(effect_genuine_95_CI_lower < effect_deepfaked_90_CI_lower, 1, 0)
  
  # binary hypothesis tests
  results_simulation <- tibble(effect_genuine_95_CI_lower   = effect_genuine_95_CI_lower,
                               effect_deepfaked_95_CI_lower = effect_deepfaked_95_CI_lower,
                               effect_deepfaked_90_CI_lower = effect_deepfaked_90_CI_lower,
                               H1_genuine                   = H1_genuine,
                               H1_deepfaked                 = H1_deepfaked,
                               H2_noninferiority            = H2_noninferiority)
  
  return(results_simulation)
  
}


run_multiple_iterations <- function(fit_initial_model_object, n_sim){
  
  results <- 
    tibble(seed = 1:n_sim) %>% 
    mutate(tidy = future_map(seed, simulate_data_update_fit_and_hypothesis_test, 
                             .options = furrr_options(seed = TRUE),
                             .progress = TRUE,
                             fit_initial_model_object = fit_initial_model_object)) %>%
    unnest(tidy) 
  
  return(results)
  
}


simulate <- function(results_file, empirical_model_fit, N_simulated_participants, iterations, initial_fit_filename){
  if(file.exists(paste0("models/", results_file, ".csv"))){
    simulation_iterations <- read.csv(paste0("models/", results_file, ".csv"))
  } else {
    # run simulations
    simulation_iterations <- empirical_model_fit %>%
      fit_initial_model(fitted_empirical_model = ., 
                        filename = initial_fit_filename, 
                        N = N_simulated_participants) %>%
      run_multiple_iterations(fit_initial_model_object = ., 
                              n_sim = iterations)
    # write to disk
    simulation_iterations %>% 
      write_csv(paste0("models/", results_file, ".csv"))
  }
  return(simulation_iterations)
}

```

# Check parameter recovery

As a sanity check to ensure that (1) models are being extracted correctly from the original (meta analytic) empirical fits, (2) data fitting these models is being simulated correctly, and (3) (fixed effect) models are being fit to these simulated data correctly, ensure that parameters from the empirical models are being recovered. Given that it's just a single simulated dataset, some degree of difference is to be expected of course.

```{r}

parameter_recovery_self_report <- 
  fit_initial_model(fitted_empirical_model = fit_selfreport, 
                    filename = "fit_simulation_initial_selfreport",
                    N = N_participants)

parameter_recovery_implicit <- 
  fit_initial_model(fitted_empirical_model = fit_implicit, 
                    filename = "fit_simulation_initial_implicit",
                    N = N_participants)

parameter_recovery_intentions <- 
  fit_initial_model(fitted_empirical_model = fit_intentions, 
                    filename = "fit_simulation_initial_intentions",
                    N = N_participants)

bind_rows(mutate(parameter_recovery_self_report$results_comparison, DV = "self report"),
          mutate(parameter_recovery_implicit$results_comparison, DV = "implicit"),
          mutate(parameter_recovery_intentions$results_comparison, DV = "intentions")) %>%
  mutate(difference = empirical - simulated) %>%
  select(DV, Parameter, empirical, simulated, difference) %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE)

```

# Run simulations

For each DV and their hypothesis. Run time of an hour or more on good hardware, depending on parameters.

```{r message=TRUE, warning=TRUE}

# full sample
simulation_iterations_self_report <- 
  simulate(results_file             = "simulation_iterations_self_report",
           initial_fit_filename     = "fit_simulation_initial_selfreport",
           empirical_model_fit      = fit_selfreport, 
           N_simulated_participants = N_participants, 
           iterations               = N_simulations)

simulation_iterations_implicit <- 
  simulate(results_file             = "simulation_iterations_implicit",
           initial_fit_filename     = "fit_simulation_initial_implicit",
           empirical_model_fit      = fit_implicit, 
           N_simulated_participants = N_participants, 
           iterations               = N_simulations)

simulation_iterations_intentions <- 
  simulate(results_file             = "simulation_iterations_intentions",
           initial_fit_filename     = "fit_simulation_initial_intentions",
           empirical_model_fit      = fit_intentions, 
           N_simulated_participants = N_participants, 
           iterations               = N_simulations)

```

## To do

NB FURTHER SIMULATIONS NEED TO BE ADDED FOR AWARENESS EXCLUSION SUBSET, AND POSSIBLE FOR COMBINED DETECTION AND AWARENESS SUBSETS. TBD BASED ON FEEDBACK FROM PREREG. IN EITHER CASE, THE SAME LOGIC OF THE EXISTING POWER ANANALYSES WOULD APPLY

```{r}

# NB simulation functions must be altered as the subset models dont compare the experiment condition variable, therefore data generation and model fits must be changed.

# # subsets
# parameter_recovery_self_report <- 
#   fit_initial_model(fitted_empirical_model = fit_selfreport_deepfaked_detected, 
#                     filename = "fit_simulation_initial_selfreport_deepfaked_detected",
#                     N = N_participants)
# 
# simulation_iterations_self_report_deefake_detected <-
#   simulation(results_file             = "simulation_iterations_self_report_deepfaked_detected",
#              initial_fit_filename     = "fit_simulation_initial_selfreport_deepfaked_detected",
#              empirical_model_fit      = fit_selfreport_deepfaked_detected,
#              N_simulated_participants = floor(N_participants*empirical_detection_rate),
#              iterations               = N_simulations)
# 
# simulation_iterations_implicit_deefake_detected <- 
#   simulation(results_file             = "simulation_iterations_implicit_detected",
#              initial_fit_filename     = "fit_simulation_initial_implicit",
#              empirical_model_fit      = fit_implicit_deepfaked_detected, 
#              N_simulated_participants = floor(N_participants*empirical_detection_rate), 
#              iterations               = N_simulations)
# 
# simulation_iterations_intentions_deefake_detected <- 
#   simulation(results_file             = "simulation_iterations_intentions_detected",
#              initial_fit_filename     = "fit_simulation_initial_intentions",
#              empirical_model_fit      = fit_intentions_deepfaked_detected, 
#              N_simulated_participants = floor(N_participants*empirical_detection_rate), 
#              iterations               = N_simulations)

```

# Results

Based on the meta-analyses of our experiments so far, and taking the exclusion rate into account (i.e., `r round(empirical_exclusion_rate*100, 1)`%), a sample size of $N \geq$ `r ceiling(N_participants / (1 - empirical_exclusion_rate))` would provide the following degree of statistical power to test each hypothesis:

NB hypothesis labels follow the preregistration document and the analysis script.

```{r fig.height=5, fig.width=12}

# combine 
simuations_iterations <- 
  bind_rows(simulation_iterations_self_report %>% mutate(DV = "Self report"),
            simulation_iterations_implicit    %>% mutate(DV = "Implicit"),
            simulation_iterations_intentions  %>% mutate(DV = "Intentions")) %>%
  mutate(DV = fct_relevel(DV, "Self report", "Implicit", "Intentions"))

# table
simuations_iterations %>%
  select(DV, H1_genuine, H1_deepfaked, H2_noninferiority) %>%
  pivot_longer(cols = c(H1_genuine, H1_deepfaked, H2_noninferiority),
               names_to = "detail",
               values_to = "detected") %>%
  group_by(DV, detail) %>%
  summarize(power = mean(detected), .groups = "drop") %>%
  mutate(hypothesis = paste(DV, detail, sep = "_"),
         hypothesis = case_when(hypothesis == "Self report_H1_genuine" ~ "H1a",
                                hypothesis == "Self report_H1_deepfaked" ~ "H1b",
                                hypothesis == "Self report_H2_noninferiority" ~ "H2a",
                                hypothesis == "Implicit_H1_genuine" ~ "H1c",
                                hypothesis == "Implicit_H1_deepfaked" ~ "H1d",
                                hypothesis == "Implicit_H2_noninferiority" ~ "H2b",
                                hypothesis == "Intentions_H1_genuine" ~ "H1e",
                                hypothesis == "Intentions_H1_deepfaked" ~ "H1f",
                                hypothesis == "Intentions_H2_noninferiority" ~ "H2c")) %>%
  select(hypothesis, DV, detail, power) %>%
  arrange(hypothesis) %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# H1 plot
simuations_iterations %>%
  select(DV, 
         Genuine   = effect_genuine_95_CI_lower,
         Deepfaked = effect_deepfaked_95_CI_lower) %>%
  pivot_longer(cols      = c(Genuine, Deepfaked),
               names_to  = "Condition",
               values_to = "value") %>%
  ggplot(aes(value, color = Condition, fill = Condition)) +
  geom_density(alpha = 0.2) +
  scale_color_viridis_d(begin = 0.3, end = 0.7) +
  scale_fill_viridis_d(begin = 0.3, end = 0.7) +
  xlab("Lower bound 95% CI") +
  facet_wrap(~ DV, scales = "free") + 
  theme(legend.position = "bottom") +
  ggtitle("H1")

# H2 plot
simuations_iterations %>%
  select(DV, 
         Genuine   = effect_genuine_95_CI_lower,
         Deepfaked = effect_deepfaked_90_CI_lower) %>%
  mutate(Inference = ifelse(Genuine < Deepfaked, "Evidence of non-inferiority", "No evidence of non-inferiority")) %>%
  ggplot(aes(Genuine, Deepfaked, color = Inference)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, linetype = "dashed") +
  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +
  xlab("Lower bound 95% CI for genuine condition") +
  ylab("Lower bound 90% CI\nfor Deepfaked condition") +
  facet_wrap(~ DV, scales = "free") + 
  theme(legend.position = "bottom") +
  ggtitle("H2")

```

# Resources

I used the [{simglm} package's vignette](https://cran.r-project.org/web/packages/simglm/vignettes/tidy_simulation.html) and [Solomon Kurz's blog post on Bayesian power analysis using {tidyverse} and {brms}](https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/) to create this simulation: many thanks to both.

# Session info

```{r}

sessionInfo()

```



