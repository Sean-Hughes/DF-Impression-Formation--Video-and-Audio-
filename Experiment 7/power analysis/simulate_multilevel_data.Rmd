


```{r}

# from https://cran.r-project.org/web/packages/simglm/vignettes/tidy_simulation.html

# dependencies
library(tidyverse)
library(simglm)
library(brms)
library(bayestestR)

# set seed
set.seed(42) 

# data_after_exclusions %>%
#   select(mean_self_reported_evaluation, source_valence, experiment_condition) %>%
#   pivot_longer(cols = c("source_valence", "experiment_condition"),
#                names_to = "IV",
#                values_to = "level") %>%
#   group_by(IV) %>%
#   summarize(mean = mean(mean_self_reported_evaluation),
#             sd = sd(mean_self_reported_evaluation)) %>%
#   mutate_if(is.numeric, round, digits = 2)

```

```{r}

# simulated sample size
N <- 1000

# which DV model should be simulated for
fit_empirical_data <- fit_selfreport

# beta estimates taken analyses of experiment 1-6
plot_model(fit_empirical_data, show.intercept = TRUE)
#plot_model(fit_selfreport, show.intercept = TRUE)


results_empirical <- as_tibble(map_estimate(fit_empirical_data))

beta_Intercept <- results_empirical %>% 
  filter(Parameter == "b_Intercept") %>% 
  pull(MAP_Estimate)

beta_source_valence <- results_empirical %>% 
  filter(Parameter == "b_source_valencepositive") %>% 
  pull(MAP_Estimate)

beta_experiment_condition <- results_empirical %>% 
  filter(Parameter == "b_experiment_conditiondeepfaked") %>%
  pull(MAP_Estimate)

beta_interaction <- results_empirical %>% 
  filter(Parameter == "b_source_valencepositive.experiment_conditiondeepfaked") %>% 
  pull(MAP_Estimate)

# set parameters of model to be simulated
simulation_arguments <- 
  list(formula     = DV ~ 1 + source_valence + experiment_condition + interaction,
       reg_weights = c(beta_source_valence, beta_source_valence, beta_experiment_condition, beta_interaction),
       fixed       = list(source_valence       = list(var_type = "continuous", mean = 0, sd = 2),
                          experiment_condition = list(var_type = "continuous", mean = 0, sd = 2),
                          interaction          = list(var_type = "continuous", mean = 0, sd = 2)),
       sample_size = N)

```

```{r}

# NB it is faster to simulate and fit the model once, then use update() this fit, rather than generating the STAN code for the model at every iteration. I'll refer to this first model as fit_initial.

# simulate data to fit_initial model
data_simulated <- simulation_arguments %>%
  simulate_fixed(data = NULL, .) %>%
  simulate_error(simulation_arguments) %>%
  generate_response(simulation_arguments)

# fit the same model to the simulated data
fit_initial <- 
  brm(formula = DV ~ 1 + source_valence + experiment_condition + interaction,
      data    = data_simulated,
      prior   = prior(normal(0, 10)),
      iter    = 10000,
      warmup  = 3000,
      control = list(adapt_delta = 0.99),  # to avoid divergent transitions
      chains  = 4,
      cores   = parallel::detectCores())

# sanity check: can we recover comparable parameters that we built the model with?
# e.g., the fixed effect estimates should be close to the reg_weights used above.
results_empirical %>%
  mutate(Parameter = case_when(Parameter == "b_source_valencepositive" ~ "b_source_valence",
                               Parameter == "b_experiment_conditiondeepfaked" ~ "b_experiment_condition",
                               Parameter == "b_source_valencepositive.experiment_conditiondeepfaked" ~ "b_interaction",
                               TRUE ~ Parameter)) %>%
  rename(empirical = MAP_Estimate) %>%
  full_join(fit_initial %>%
              map_estimate() %>%
              as_tibble() %>%
              rename(simulated = MAP_Estimate), by = "Parameter") %>%
  mutate_if(is.numeric, round, digits = 2)

# fit <- lm(DV ~ source_valence + experiment_condition + interaction, 
#           data = data_simulated)
# summary(fit)

```

```{r}

# simulate data to refit the model (via updating) using the initial model and newly simulated data
data_simulated <- sim_arguments %>%
  simulate_fixed(data = NULL, .) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)

# find a new (updated) fit using the newly simulated data
fit_updated <- update(fit_initial, newdata = data_simulated)

# [NB below hypothesis testing code adapted from analysis code]

# manipulate posterior distributions
draws_fit_updated <-
  bind_cols(select(spread_draws(fit_updated, b_Intercept),            b_Intercept),
            select(spread_draws(fit_updated, b_source_valence),       b_source_valence),
            select(spread_draws(fit_updated, b_experiment_condition), b_experiment_condition),
            select(spread_draws(fit_updated, b_interaction),          b_interaction)) %>%
  mutate(effect_genuine   = b_Intercept + b_source_valence,
         effect_deepfaked = b_Intercept + b_source_valence + b_experiment_condition + b_interaction)

# parameterize posteriors
estimates_fit_updated <-
  # 95% CIs
  (bayestestR::hdi(draws_fit_updated, ci = .95) %>% 
     rename(CI_95_lower = CI_low) %>%
              as_tibble()) %>%
  # 90% CIs (for non-inferiority test)
  full_join(bayestestR::hdi(draws_fit_updated, ci = .90) %>%
              as_tibble() %>%
              rename(CI_90_lower = CI_low),
            by = "Parameter") %>%
  select(Parameter, CI_95_lower, CI_90_lower)

# hypothesis tests
## non-zero effect in genuine condition
H1ace <- ifelse((estimates_fit_updated %>% filter(Parameter == "effect_genuine") %>% pull(CI_95_lower)) > 0, 1, 0)
## non-zero effect in deepfake condition
H1bdf <- ifelse((estimates_fit_updated %>% filter(Parameter == "effect_deepfaked") %>% pull(CI_95_lower)) > 0, 1, 0)
## non-inferiority of deepfakes compared to genuine
H2abc <- ifelse((estimates_fit_updated %>% filter(Parameter == "effect_deepfaked") %>% pull(CI_90_lower)) > 
                  (estimates_fit_updated %>% filter(Parameter == "effect_genuine") %>% pull(CI_95_lower)), 1, 0)

# binary hypothesis tests
results_simulation <- tibble(H1ace = H1ace,
                             H1bdf = H1bdf,
                             H2abc = H2abc)

# print
results_simulation

```




